{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a08e596-f84d-44cc-b45e-f9dac516e117",
   "metadata": {},
   "source": [
    "# makemore 4: becoming a backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e35c06cf-f43b-43a1-a762-00b7b990f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there no change change in the first several cells from last lecture\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8f8f433c-1648-4674-9b10-f4d524569876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "58dbcfaf-2782-48e7-831a-d5b116c6ee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e2f7f945-936f-4159-b4dc-696bd33cef69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "86927819-ae95-4164-b9c8-7e68b905e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "0f5eabbb-a30d-4ad3-bc57-4f96de1275cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3cf27ef0-79c0-4ecc-bc3b-99695b1420f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0c42ae54-cecf-4ac9-914c-151b178e3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "befb3cf9-613f-4e04-9ca7-42505ab7ba09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3427, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a87b30e9-761b-4068-836e-1567d8fb94fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([27, 10]), torch.Size([32, 3, 10]))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.shape, C.shape, demb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a225a8a8-0f2a-4c76-9881-d67c6a53ed09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 10])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demb.view(-1, demb.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1ec7102f-f5a3-4e45-a099-8727147f4631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# initialize the gradient\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dprobs = torch.zeros_like(probs)\n",
    "dcounts_sum_inv = torch.zeros_like(counts_sum_inv)\n",
    "dcounts = torch.zeros_like(counts)\n",
    "dcounts_sum = torch.zeros_like(counts_sum)\n",
    "dnorm_logits = torch.zeros_like(norm_logits)\n",
    "dlogit_maxes = torch.zeros_like(logit_maxes)\n",
    "dlogits = torch.zeros_like(logits)\n",
    "dh = torch.zeros_like(h)\n",
    "dW2 = torch.zeros_like(W2)\n",
    "db2 = torch.zeros_like(b2)\n",
    "dhpreact = torch.zeros_like(hpreact)\n",
    "dbngain = torch.zeros_like(bngain)\n",
    "dbnraw = torch.zeros_like(bnraw)\n",
    "dbnbias = torch.zeros_like(bnbias)\n",
    "dbndiff = torch.zeros_like(bndiff)\n",
    "dbnvar_inv = torch.zeros_like(bnvar_inv)\n",
    "dbnvar = torch.zeros_like(bnvar)\n",
    "dbndiff2 = torch.zeros_like(bndiff2)\n",
    "dhprebn = torch.zeros_like(hprebn)\n",
    "dbnmeani = torch.zeros_like(bnmeani)\n",
    "dembcat = torch.zeros_like(embcat)\n",
    "dW1 = torch.zeros_like(W1)\n",
    "db1 = torch.zeros_like(b1)\n",
    "demb = torch.zeros_like(emb)\n",
    "dC = torch.zeros_like(C)\n",
    "\n",
    "\n",
    "# cross entropy loss backward\n",
    "dlogprobs[range(n), Yb] += -(logprobs.shape[0])**-1\n",
    "dprobs += probs**-1 * dlogprobs\n",
    "dcounts_sum_inv += (counts * dprobs).sum(dim=1, keepdim=True)\n",
    "dcounts += counts_sum_inv * dprobs\n",
    "dcounts_sum += - (counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += dcounts_sum * torch.ones_like(counts)\n",
    "dnorm_logits += counts * dcounts\n",
    "dlogits += dnorm_logits.clone()\n",
    "dlogit_maxes += - dnorm_logits.sum(dim=1, keepdim=True)\n",
    "dlogits += dlogit_maxes * F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]).float()\n",
    "# Linear layer 2 backward\n",
    "dh += dlogits @ W2.T\n",
    "dW2 += h.T @ dlogits\n",
    "db2 += dlogits.sum(dim=0)\n",
    "# Non-linearity backward\n",
    "dhpreact += (1.0 - h**2) * dh\n",
    "# BatchNorm layer backward\n",
    "dbngain += (bnraw * dhpreact).sum(dim=0, keepdim=True)\n",
    "dbnraw += bngain * dhpreact\n",
    "dbnbias += dhpreact.sum(dim=0, keepdim=True)\n",
    "dbndiff += bnvar_inv * dbnraw\n",
    "dbnvar_inv += (bndiff * dbnraw).sum(dim=0, keepdim=True)\n",
    "dbnvar += -0.5 * (bnvar + 1e-5)**(-1.5) * dbnvar_inv \n",
    "dbndiff2 += 1.0/(n-1)*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += 2.0 * bndiff * dbndiff2\n",
    "dbnmeani += - dbndiff.sum(dim=0, keepdim=True)\n",
    "dhprebn += dbndiff.clone()\n",
    "dhprebn += 1.0 /n * dbnmeani\n",
    "# Linear layer 1 backward\n",
    "dembcat += dhprebn @ W1.T\n",
    "dW1 += embcat.T @ dhprebn\n",
    "db1 += dhprebn.sum(dim=0)\n",
    "demb += dembcat.view(emb.shape)\n",
    "dC += F.one_hot(Xb, num_classes=27).float().view(-1, C.shape[0]).T @ demb.view(-1, demb.shape[-1])\n",
    "\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "730346f7-4793-434e-bff5-937f8b9db5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.342686653137207 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9cd1ac09-ad3e-4e5f-b8fd-4ab09dee58a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape,  F.one_hot(Yb, num_classes=logits.shape[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "63410430-b729-4a66-85fb-799a29a33874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 4.889443516731262e-09\n"
     ]
    }
   ],
   "source": [
    "# backward path\n",
    "\n",
    "dlogits = F.softmax(logits, dim=1)\n",
    "# dlogits = logits.exp() / logits.exp().sum(dim=1, keepdim=True)\n",
    "\n",
    "dlogits += - F.one_hot(Yb, num_classes=logits.shape[1]).float()\n",
    "dlogits *= 1/n\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4a800855-db14-4966-8df1-2a0f567140ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1494fd9c0>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f8ce26fc-c128-4357-a0f3-361ce58f0c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "722c8af5-a23f-42c4-977d-0f878585c5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "048bd313-ac9b-4828-8d09-63db5e090f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7369\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# # use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "    for i in range(max_steps):\n",
    "    \n",
    "        # minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    \n",
    "        # forward pass\n",
    "        emb = C[Xb] # embed the characters into vectors\n",
    "        embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "        # Linear layer\n",
    "        hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "        # BatchNorm layer\n",
    "        # -------------------------------------------------------------\n",
    "        bnmean = hprebn.mean(0, keepdim=True)\n",
    "        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "        bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        # -------------------------------------------------------------\n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact) # hidden layer\n",
    "        logits = h @ W2 + b2 # output layer\n",
    "        loss = F.cross_entropy(logits, Yb) # loss function\n",
    "    \n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "          p.grad = None\n",
    "        # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "    \n",
    "    \n",
    "        # manual backprop! #swole_doge_meme\n",
    "        # -----------------\n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(n), Yb] -= 1\n",
    "        dlogits /= n\n",
    "        # 2nd layer backprop\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "        # tanh\n",
    "        dhpreact = (1.0 - h**2) * dh\n",
    "        # batchnorm backprop\n",
    "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "        dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "        # 1st layer\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        # embedding\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = F.one_hot(Xb, num_classes=27).float().view(-1, C.shape[0]).T @ demb.view(-1, demb.shape[-1])\n",
    "              \n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "        # # -----------------\n",
    "    \n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "        for p, grad in zip(parameters, grads):\n",
    "          # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "          p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "    \n",
    "        # track stats\n",
    "        if i % 10000 == 0: # print every once in a while\n",
    "          print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        lossi.append(loss.log10().item())\n",
    "    \n",
    "        if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f538bc64-364b-45b2-abad-037460a5bc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 10)        | exact: False | approximate: True  | maxdiff: 9.778887033462524e-09\n",
      "(30, 200)       | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "(200,)          | exact: False | approximate: True  | maxdiff: 4.190951585769653e-09\n",
      "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "(27,)           | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "for p,g in zip(parameters, grads):\n",
    "  cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a91c4050-a95b-449e-8459-790cf0511c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b0bc35e2-1671-492a-a599-e6c97b423b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0703086853027344\n",
      "val 2.110659122467041\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5022462-232c-4764-b606-5e6b81ac84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto grad\n",
    "# train 2.0716941356658936\n",
    "# val 2.1107215881347656\n",
    "\n",
    "# manual grad\n",
    "# train 2.0703086853027344\n",
    "# val 2.110659122467041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f7686-e2e7-472b-b091-7673dbfd9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_learning_venv)",
   "language": "python",
   "name": "ml_learning_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
